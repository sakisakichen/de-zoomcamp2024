{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def load_data_from_api(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template for loading data from API\n",
    "    \"\"\"\n",
    "    url_202010 = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-10.csv.gz'\n",
    "    url_202011 = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-11.csv.gz'\n",
    "    url_202012 = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2020-12.csv.gz'\n",
    "    # DEFINE A DICTIONARY OF DATA TYPES FOR THE NON DATETIME COLUMNS \n",
    "    taxi_dtypes = {\n",
    "        'VendorID':pd.Int64Dtype(),\n",
    "        'store_and_fwd_flag':str,\n",
    "        'RatecodeID':pd.Int64Dtype(),\n",
    "        'PULocationID':pd.Int64Dtype(),\n",
    "        'DOLocationID':pd.Int64Dtype(),\n",
    "        'passenger_count':pd.Int64Dtype(),\n",
    "        'trip_distance':float,\n",
    "        'fare_amount':float,\n",
    "        'extra':float,\n",
    "        'mta_tax':float,\n",
    "        'tip_amount':float,\n",
    "        'tolls_amount':float,\n",
    "        'ehail_fee':float,\n",
    "        'improvement_surcharge':float,\n",
    "        'total_amount':float,\n",
    "        'payment_type':float,\n",
    "        'trip_type':float,\n",
    "        'congestion_surcharge':float\n",
    "    }\n",
    "\n",
    "    # CREATE A LIST OF DATETIME COLUMNS.\n",
    "    # The list will be passed to the read_csv function and pandas will parse the columns as dates with the appropriate time stamps.  \n",
    "    parse_dates = ['lpep_pickup_datetime', 'lpep_dropoff_datetime']  \n",
    "\n",
    "    # read_csv LOADS A CSV FILE INTO A DATAFRAME. THIS BLOCK RETURNS THAT DF. \n",
    "    df1 = pd.read_csv(url_202010, sep=',', compression=\"gzip\", dtype=taxi_dtypes, parse_dates=parse_dates)\n",
    "    df2 = pd.read_csv(url_202011, sep=',', compression=\"gzip\", dtype=taxi_dtypes, parse_dates=parse_dates)\n",
    "    df3 = pd.read_csv(url_202012, sep=',', compression=\"gzip\", dtype=taxi_dtypes, parse_dates=parse_dates)\n",
    "    combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "    # return pd.read_csv(url_202010, sep=',', compression=\"gzip\", dtype=taxi_dtypes, parse_dates=parse_dates)\n",
    "    return combined_df\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# import os\n",
    "\n",
    "if 'transformer' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import transformer\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@transformer\n",
    "def transform(data, *args, **kwargs):\n",
    "    # PRINT COUNTS OF RECORDS WITH \n",
    "    # print(f\"Preprocessing: rows with zero passengers:{data['passenger_count'].isin([0]).sum()}\")\n",
    "\n",
    "    # RETURN FILTERED DATA SET\n",
    "    data = data[(data['passenger_count']>0) & (data['trip_distance']> 0) ]\n",
    "    data['lpep_pickup_date '] = data['lpep_pickup_datetime'].dt.date\n",
    "    data = data.rename(columns={'VendorID': 'vendor_id'})\n",
    "    # # data.rename(columns = {'VenderID'}:{'vender_id'},inplace = True)\n",
    "    # # data = data.rename(columns={\"VenderID\": \"vender_id\"},inplace = True)\n",
    "\n",
    "    data.columns = (data.columns\n",
    "                    .str.replace(' ', '_')\n",
    "                    .str.lower()\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@test\n",
    "# CHECK THAT THERE ARE NO RECORDS WITH 0 PASSENGER COUNT\n",
    "def test_output(output, *args):\n",
    "    assert output['passenger_count'].all()> 0, 'There are rides with zero passengers'\n",
    "    assert (output['trip_distance'] > 0).all()  \n",
    "    assert 'vendor_id' in list(output.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "\n",
    "\n",
    "# MANUALLY DEFINE THE CREDENTIALS\n",
    "# Set the environment variable to the location of the mounted key. json\n",
    "# This will tell pyarrow where our credentials are\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/home/src/de-project-datatalksclub-23996505fb10.json\"\n",
    "\n",
    "# Define the bucket, project, and table  \n",
    "bucket_name = 'mage-zoomcamp-saki-001'\n",
    "project_id = 'de-project-datatalksclub'\n",
    "table_name = 'green_taxi'          \n",
    "\n",
    "root_path = f'{bucket_name}/{table_name}'\n",
    "\n",
    "@data_exporter\n",
    "def export_data(data, *args, **kwargs):\n",
    "    # define the column to partition on \n",
    "    # create a date column from the timestamp so that we can partition on date\n",
    "    data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date\n",
    "\n",
    "    # define the pyarrow table and read the df into it\n",
    "    table = pa.Table.from_pandas(data)\n",
    "\n",
    "    # define file system - the google cloud object that is going to authorize using the environmental variable automatically\n",
    "    gcs = pa.fs.GcsFileSystem()\n",
    "\n",
    "    # write to the dataset using a parquet function\n",
    "    pq.write_to_dataset(\n",
    "        table, \n",
    "        root_path=root_path, \n",
    "        partition_cols=['lpep_pickup_date'], # needs to be a list\n",
    "        filesystem=gcs\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
